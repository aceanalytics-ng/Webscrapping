{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e75cd0a",
   "metadata": {},
   "source": [
    "# Web Scraping in Python using Beautiful Soup\n",
    "### *By:* *`Ayobami Yusuf`*\n",
    "### **Introduction:**\n",
    "> This is a simple project/tutorial that seeks to explain how to programmatically **scrape** (extract) data from the **web** (internet) - a task generally refered to as **Web Scraping** - in python using two modules (***`requests`*** and ***`BeautifulSoup`***). <p> This notebook is structured in an easy-to-follow manner to enable begeinners fully gain the knowledge and skills required to successfully complete a web scraping task. <p> For this project, we will be accessing and extracting the **[IMDb Top 250 Movies of all Time data](https://www.imdb.com/chart/top)** and load the data into an excel file for analytical purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bffcad",
   "metadata": {},
   "source": [
    "## Packages/Libraries/Modules Installations\n",
    "\n",
    "> Apart from `BeautifulSoup`, and the popular `pandas` libraries, because we intend to load our extracted data into an excel file for further processing or analyses, we would be using a cool library for this task - **`openpyxl.`** Again, this tutorial does not cover any introduction to openpyxl but **[here's openpyxl's documentation](https://openpyxl.readthedocs.io/en/stable/)** for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a6244f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #to access the website's html contents\n",
    "from bs4 import BeautifulSoup #to parse the contents from the accessed html page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f48d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IMDb Ratings']\n"
     ]
    }
   ],
   "source": [
    "#import the openpyxl library\n",
    "import openpyxl as pxl\n",
    "\n",
    "#create an excel file that will contain the excel worksheet to store the data in\n",
    "file = pxl.Workbook()\n",
    "\n",
    "#activate the current(open/active) worksheet as the sheet to be used\n",
    "sheet = file.active\n",
    "\n",
    "#rename/retitle the activated worksheet\n",
    "sheet.title = \"IMDb Ratings\"\n",
    "\n",
    "#create the column headers\n",
    "sheet.append([\"rank\", \"title\", \"release_year\", \"imdb_rating\"])\n",
    "\n",
    "print(file.sheetnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7a3c1",
   "metadata": {},
   "source": [
    "## Connect to the webpage that houses the needed data and extract the data from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4246eeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<meta charset=\"utf-8\"/>,\n",
       " <script type=\"text/javascript\">var IMDbTimer={starttime: new Date().getTime(),pt:'java'};</script>,\n",
       " <script>\n",
       "     if (typeof uet == 'function') {\n",
       "       uet(\"bb\", \"LoadTitle\", {wb: 1});\n",
       "     }\n",
       " </script>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#url to the page to be accessed:\n",
    "url = \"https://www.imdb.com/chart/top\"\n",
    "\n",
    "#sends a request(to grant access) to the server hosting the page and returns a response object \n",
    "#that confirms access to the page (ie THE SOURCE CODE OF THE PAGE) and also some status information (200 means 'success')\n",
    "source_code = requests.get(url) \n",
    "\n",
    "#parses the retrived contents(source code alone) using BeautifulSoup via the lxml parser\n",
    "soup = BeautifulSoup(source_code.text, 'lxml') #the .text method is needed to retrieve ONLY the html source code\n",
    "\n",
    "#now, lets take just a sneak peak into the html contents retrived\n",
    "soup.head()[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b272b7f",
   "metadata": {},
   "source": [
    "Beautiful! (No pun intended), our soup has been beautifully prepared (still, no pun intended...well, maybe a little). We have successfully accessed the page's front end and retrievd it's source code and parsed it into a \"pythonable\" format using the popular ***`lxml`*** parser. By the way, **[here's](https://www.scrapingbee.com/blog/data-parsing/)** a good read to understand the concept of **[parsing/parser/parse](https://www.scrapingbee.com/blog/data-parsing/)** and why they are needed. A little (very little) potion of the content is shown to confirm that we truly have successfully completed this phase.\n",
    "\n",
    "## Next up, we retrieve the actual data needed from this page\n",
    "> Most of the time, we do not need all the information on the page we are trying to scrape from. Most data analytics operations requiring web scraping to be done usually need to access and retrieve ONLY data tables found on web pages like the one found on IMDb Top 250 Movies page that shows the **Rank, Title, Release Year,** and **IMDb Rating** of 250 movies considered as the Top 250 movies of all time. <p> Now this is where basic knowledge of HTML and HTML tags are very useful (not compulsory). For this project/tutorial, we won't be covering any introduction to HTML. You can refer to **[W3 Schools' HTML tutorial](https://www.w3schools.com/html/html_intro.asp)** to learn about the basics of HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2a2faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data table we need can be found in the <tbody> tag with the (lister-list) class attribute\n",
    "#and the data are found in the <tr> children tags of the <tbody> parent tag. \n",
    "#we use this knowledge to locate and extract the data\n",
    "movie_table = soup.find('tbody', class_='lister-list').find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd8e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have located all the tags that houses our data\n",
    "#we can write a for loop that iterates through each <tr> tag and extracts all the needed data in each automatically\n",
    "for movie in movie_table:\n",
    "    rank = movie.find('td', class_='titleColumn').get_text(strip=True).split('.')[0]\n",
    "    title = movie.find('td', class_='titleColumn').a.text\n",
    "    year = movie.find('td', class_='titleColumn').span.text.strip('()')\n",
    "    rating = movie.find('td', class_='ratingColumn imdbRating').text.replace('\\n','')\n",
    "    \n",
    "    #since we have our worksheet set up already, we can append the data extracted to the open sheet directly from the loop\n",
    "    sheet.append([rank, title, year, rating])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b5bf9",
   "metadata": {},
   "source": [
    "## Next, we save the Excel file to local machine memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f6b02df",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save(\"IMDB Ratings Data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314e67e",
   "metadata": {},
   "source": [
    "### Finally, to assess all that we have done, we can load the saved excel file into our notebook environment to be sure the data were truly scraped and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f725d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>title</th>\n",
       "      <th>release_year</th>\n",
       "      <th>imdb_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>1974</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>1957</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank                     title  release_year  imdb_rating\n",
       "0     1  The Shawshank Redemption          1994          9.2\n",
       "1     2             The Godfather          1972          9.2\n",
       "2     3           The Dark Knight          2008          9.0\n",
       "3     4     The Godfather Part II          1974          9.0\n",
       "4     5              12 Angry Men          1957          9.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_excel(\"IMDB Ratings Data.xlsx\")\n",
    "\n",
    "#view the first five rows of the excel worsheet\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc9b628",
   "metadata": {},
   "source": [
    "## Tasks Completed.\n",
    "\n",
    "And, that's it. You should see the worksheet on your computer (on the same folder as your jupyter notebok or your .py file). If you find this useful, or have any questions regarding web scraping, data engineering, or data analysis, kindly reach out to me on **[www.linkedin.com/in/ayobami-yusuf](www.linkedin.com/in/ayobami-yusuf)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
